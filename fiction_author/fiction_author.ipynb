{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intellectual-procurement",
   "metadata": {},
   "source": [
    "# 사용된 XGBoost Feature\n",
    "\n",
    "- Meta Feature (문장 길이, Stop words 갯수, ..., Named Entity)\n",
    "- FastText Embedding\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- SGDClassifier\n",
    "- RandomForestClassifier\n",
    "- MLPClassifier\n",
    "- DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "documented-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "data_path = '../../kaggle_data/fiction_author/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv', encoding='utf-8')\n",
    "test = pd.read_csv(data_path + 'test_x.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suburban-substitute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  author\n",
       "0      0  He was almost choking. There was so much, so m...       3\n",
       "1      1             “Your sister asked for it, I suppose?”       2\n",
       "2      2   She was engaged one day as she walked, in per...       1\n",
       "3      3  The captain was in the porch, keeping himself ...       4\n",
       "4      4  “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bridal-danish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text\n",
       "0      0  “Not at all. I think she is one of the most ch...\n",
       "1      1  \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      2  As the lady had stated her intention of scream...\n",
       "3      3  “And then suddenly in the silence I heard a so...\n",
       "4      4  His conviction remained unchanged. So far as I..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-pride",
   "metadata": {},
   "source": [
    "### textstat\n",
    "\n",
    "- Textstat는 텍스트에서 통계를 계산하는 데 사용하기 쉬운 라이브러리입니다. 가독성, 복잡성 및 등급 수준을 결정하는 데 도움이 됩니다.\n",
    "\n",
    "### fasttext\n",
    "\n",
    "- 단어를 벡터로 만드는 또 다른 방법으로는 페이스북에서 개발한 FastText가 있습니다.\n",
    "- Word2Vec 이후에 나온 것이기 때문에, 메커니즘 자체는 Word2Vec의 확장이라고 볼 수 있습니다. \n",
    "- Word2Vec와 FastText와의 가장 큰 차이점이라면 Word2Vec는 단어를 쪼개질 수 없는 단위로 생각한다면, FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주합니다. 즉 내부 단어(subword)를 고려하여 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "monetary-mexico",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textstat in c:\\users\\marti\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: pyphen in c:\\users\\marti\\anaconda3\\lib\\site-packages (from textstat) (0.10.0)\n",
      "Requirement already satisfied: fasttext in c:\\users\\marti\\anaconda3\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\marti\\anaconda3\\lib\\site-packages (from fasttext) (1.19.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\marti\\anaconda3\\lib\\site-packages (from fasttext) (52.0.0.post20210125)\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\marti\\anaconda3\\lib\\site-packages (from fasttext) (2.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat\n",
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-cleveland",
   "metadata": {},
   "source": [
    "## 1) flesch_reading_ease\n",
    "\n",
    "- Flesch reading-ease test 에서는 점수가 높을수록 읽기 쉬운 재료를 나타내며 숫자가 낮을수록 읽기 어려운 구절을 표시합니다.\n",
    "- Flesch Reading Ease Score를 반환합니다.\n",
    "- 최대 점수는 121.22점이지만 점수가 얼마나 낮을 수 있는지에 대한 제한은 없습니다. 음수 점수가 유효합니다.\n",
    "\n",
    "## 2) nltk\n",
    "\n",
    "- 교육용으로 개발된 자연어 처리 및 문서 분석용 파이썬 패키지. 다양한 기능 및 예제를 가지고 있으며 실무 및 연구에서도 많이 사용됩니다.\n",
    "- 말뭉치(corpus)는 자연어 분석 작업을 위해 만든 샘플 문서 집합을 말한다. 단순히 소설, 신문 등의 문서를 모아놓은 것도 있지만 품사. 형태소, 등의 보조적 의미를 추가하고 쉬운 분석을 위해 구조적인 형태로 정리해 놓은 것을 포함한다.\n",
    "- 말뭉치 자료는 설치시에 제공되지 않고 download 명령으로 사용자가 다운로드 받아야 한다.\n",
    "- nltk.download(\"book\") 명령을 실행하면 NLTK 패키지 사용자 설명서에서 요구하는 대부분의 말뭉치를 다운로드 받아준다.\n",
    "\n",
    "### 2-1) nltk.tokenize\n",
    "\n",
    "- 자연어 문서를 분석하기 위해서는 우선 긴 문자열을 분석을 위한 작은 단위로 나누어야 한다. 이 문자열 단위를 토큰(token)이라고 하고 이렇게 문자열을 토큰으로 나누는 작업을 토큰 생성(tokenizing)이라고 한다. 영문의 경우에는 문장, 단어 등을 토큰으로 사용하거나 정규 표현식을 쓸 수 있다.\n",
    "- 문자열을 토큰으로 분리하는 함수를 토큰 생성 함수(tokenizer)라고 한다. 토큰 생성 함수는 문자열을 입력받아 토큰 문자열의 리스트를 출력한다.\n",
    "~~~python\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(emma_raw[50:100])\n",
    "~~~\n",
    "> ['Emma',\n",
    " 'Woodhouse',\n",
    " ',',\n",
    " 'handsome',\n",
    " ',',\n",
    " 'clever',\n",
    " ',',\n",
    " 'and',\n",
    " 'rich',\n",
    " ',',\n",
    " 'with',\n",
    " 'a']\n",
    " \n",
    "### 2-2) from nltk.tag import pos_tag (품사 부착)\n",
    " \n",
    " - 품사(POS, part-of-speech)는 낱말을 문법적인 기능이나 형태, 뜻에 따라 구분한 것이다. 품사의 구분은 언어마다 그리고 학자마다 다르다. 예를 들어 NLTK에서는 펜 트리뱅크 태그세트(Penn Treebank Tagset)라는 것을 이용한다. 다음은 펜 트리뱅크 태그세트에서 사용하는 품사의 예이다.\n",
    "     - NNP : 단수 고유명사\n",
    "     - VB : 동사\n",
    "     - VBP : 동사 현재형\n",
    "     - TO : to 전칳사\n",
    "     - NN : 명사(단수형 혹은 집합형)\n",
    "     - DT : 관형사\n",
    " - pos_tag 명령을 사용하면 단어 토큰에 품사를 부착하여 튜플로 출력한다. 다음 예문에서 refuse, permit이라는 같은 철자의 단어가 각각 동사와 명사로 다르게 품사 부착된 것을 볼 수 있다.\n",
    " ~~~python\n",
    "from nltk.tag import pos_tag\n",
    "sentence = \"Emma refused to permit us to obtain the refuse permit\"\n",
    "tagged_list = pos_tag(word_tokenize(sentence))\n",
    "tagged_list\n",
    "~~~\n",
    "> [('Emma', 'NNP'),\n",
    " ('refused', 'VBD'),\n",
    " ('to', 'TO'),\n",
    " ('permit', 'VB'),\n",
    " ('us', 'PRP'),\n",
    " ('to', 'TO'),\n",
    " ('obtain', 'VB'),\n",
    " ('the', 'DT'),\n",
    " ('refuse', 'NN'),\n",
    " ('permit', 'NN')]\n",
    " \n",
    " - Scikit-Learn 등에서 자연어 분석을 할 때는 같은 토큰이라도 품사가 다르면 다른 토큰으로 처리해야 하는 경우가 많은데 이 때는 원래의 토큰과 품사를 붙여서 새로운 토큰 이름을 만들어 사용하면 철자가 같고 품사가 다른 단어를 구분할 수 있다.\n",
    " \n",
    "### 2-3) nltk.ne_chunk\n",
    "\n",
    "- nltk 라이브러리 ne_chunk() 함수를 사용해서 개체명을 인식시킬 수 있다\n",
    "- 개체명 인식을 사용하면 코퍼스로부터 어떤 단어가 사람, 장소, 조직 등을 의미하는 단어인지를 찾을 수 있습니다.\n",
    "- 어떤 이름을 의미하는 단어를 보고는 그 단어가 어떤 유형인지를 인식하는 것을 말합니다.\n",
    "    - \"유정이는 2018년에 골드만삭스에 입사했다.\"\n",
    "    - 유정 -> 사람 / 2018년 -> 시간 / 골드만삭스 -> 조직\n",
    "- NLTK에서는 개체명 인식기(NER chunker)를 지원하고 있으므로, 별도 개체명 인식기를 구현할 필요없이 NLTK를 사용해서 개체명 인식을 수행할 수 있습니다.\n",
    "- ne_chunk는 개체명을 태깅하기 위해서 앞서 품사 태깅(pos_tag)이 수행되어야 합니다. 위의 결과에서 James는 PERSON(사람), Disney는 조직(ORGANIZATION), London은 위치(GPE)라고 정상적으로 개체명 인식이 수행된 것을 볼 수 있습니다.\n",
    "~~~python\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "sentence = \"James is working at Disney in London\"\n",
    "sentence=pos_tag(word_tokenize(sentence))\n",
    "print(sentence) # 토큰화와 품사 태깅을 동시 수행\n",
    "~~~\n",
    "> [('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n",
    "~~~python\n",
    "sentence=ne_chunk(sentence)\n",
    "print(sentence) # 개체명 인식\n",
    "~~~\n",
    "> (S\n",
    "  (PERSON James/NNP)\n",
    "  is/VBZ\n",
    "  working/VBG\n",
    "  at/IN\n",
    "  (ORGANIZATION Disney/NNP)\n",
    "  in/IN\n",
    "  (GPE London/NNP))\n",
    "\n",
    "### 2-4) from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "- 문자열을 가져와서 네 가지 범주 각각에 대한 점수 dictionary를 반환합니다.\n",
    "    - negative\n",
    "    - neutral\n",
    "    - positive\n",
    "    - compound(computed by normalizing the scores above)\n",
    "    \n",
    "> a = 'This was a good movie.'\n",
    "> sid.polarity_scores(a)\n",
    "\n",
    "> OUTPUT-{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
    "\n",
    "> a = 'This was the best, most awesome movie EVER MADE!!!'\n",
    "> sid.polarity_scores(a)\n",
    "\n",
    "> OUTPUT-{'neg': 0.0, 'neu': 0.425, 'pos': 0.575, 'compound': 0.8877}\n",
    "\n",
    "### 2-5) from nltk.corpus import stopwords\n",
    "\n",
    "- 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요합니다. \n",
    "- 여기서 큰 의미가 없다라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들을 말합니다. \n",
    "- 예를 들면, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우가 있습니다. \n",
    "- 이러한 단어들을 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다.\n",
    "~~~python\n",
    "from nltk.corpus import stopwords  \n",
    "stopwords.words('english')[:10]\n",
    "~~~\n",
    "> ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dominant-description",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textstat import flesch_reading_ease\n",
    "\n",
    "import fasttext\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, tree2conlltags\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import metrics, model_selection, naive_bayes\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "symbols_knowns = string.ascii_letters + string.digits + string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-field",
   "metadata": {},
   "source": [
    "- punctuation ; 따옴표, 마침표, 물음표 등등 과 같은 문장부호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prostate-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_nltk(text):\n",
    "    res = SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "    return res['compound']\n",
    "\n",
    "def get_words(text):\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    return [word for word in words if not word in string.punctuation]\n",
    "    \n",
    "def count_tokens(text, tokens):\n",
    "    return sum([w in tokens for w in get_words(text)])\n",
    "\n",
    "def first_word_len(text):\n",
    "    if(len(get_words(text))==0):\n",
    "        return 0\n",
    "    else:   \n",
    "        return len(get_words(text)[0])\n",
    "\n",
    "def last_word_len(text):\n",
    "    if(len(get_words(text))==0):\n",
    "        return 0\n",
    "    else:   \n",
    "        return len(get_words(text)[-1])\n",
    "\n",
    "def symbol_id(x):\n",
    "    symbols=[x for x in symbols_knowns]\n",
    "      \n",
    "    if x not in symbols:\n",
    "        return -1 \n",
    "    else:\n",
    "        return np.where(np.array(symbols) == x )[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cardiovascular-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전체 단어 개수에서 '명사'의 품사를 가진 단어의 비율을 구함.\n",
    "def fraction_noun(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__() # len()과 object.__len__()은 다름. len()은 __len__()을 호출하는 것임.\n",
    "    if word_count==0:\n",
    "        return 0\n",
    "    else:\n",
    "        pos_list = nltk.pos_tag(text_splited)\n",
    "        noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
    "    \n",
    "        return (noun_count/word_count)\n",
    "    \n",
    "## 전체 단어 개수에서 '형용사'의 품사를 가진 단어의 비율을 구함.\n",
    "def fraction_adj(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    if word_count==0:\n",
    "        return 0\n",
    "    else:\n",
    "        pos_list = nltk.pos_tag(text_splited)\n",
    "        adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
    "    \n",
    "        return (adj_count/word_count)  \n",
    "\n",
    "## 전체 단어 개수에서 '동사'의 품사를 가진 단어의 비율을 구함.\n",
    "def fraction_verbs(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    if word_count==0:\n",
    "        return 0\n",
    "    else:\n",
    "        pos_list = nltk.pos_tag(text_splited)\n",
    "        verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
    "    \n",
    "        return (verbs_count/word_count)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-engineering",
   "metadata": {},
   "source": [
    "## 생성한 feature\n",
    "\n",
    "1. 각 문장에 포함된 **'단어의 개수'**\n",
    "2. 각 문장에 포함된 **'단어의 평균 길이'**\n",
    "3. 각 문장에 포함된 **'겹치지 않는 단어의 개수'**\n",
    "4. 각 문장에 포함된 **'문자의 개수'**\n",
    "5. 각 문장에 포함된 **'stopwards(불용어)의 개수'**\n",
    "6. 각 문장에 포함된 **'문장부호의 개수'**\n",
    "7. 각 문자에 포함된 단어 중 **'Upper case로 된 단어의 비율'**\n",
    "8. 각 문자에 포함된 단어 중 **'title case(upper case + lower case)로 된 단어의 비율'**\n",
    "9. 각 문장에 포함된 전체 문자의 개수 중 **'','로 구분되어진 chunk에 포함된 문자들의 평균 개수에 대한 비율'**\n",
    "10. 각 문장에 포함된 전체 문자의 개수 중 **'ascii 문자나 숫자와 같은 symbol의 비율'**\n",
    "11. 각 문장에 포함된 **'명사의 개수'**\n",
    "12. 각 문장에 포함된 **'형용사의 개수'**\n",
    "13. 각 문장에 포함된 **'동사의 개수'**\n",
    "14. 각 문장의 **'SentimentIntensityAnalyzer의 compound 분석 값'**\n",
    "15. 각 문장에서 **'단수 주어/주어/목적어 token이 포함된 갯수'**\n",
    "16. 각 문장에서 **'복수 주어/주어/목적어 token이 포함된 갯수'**\n",
    "17. 각 문장에 포함된 전체 문자의 개수에 대한 **'첫번째 문자 길이의 비율'**\n",
    "18. 각 문장에 포함된 전체 문자의 개수에 대한 **'마지막 문자 길이의 비율'**\n",
    "19. **첫번째 단어의 'symbol id를 구함'**\n",
    "20. **마지막 단어의 'symbol id를 구함'**\n",
    "21. **flesch_reading_ease score**를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "twenty-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장에 포함된 '단어의 개수'\n",
    "train['num_words']=train['text'].apply(lambda x:len(get_words(x)))\n",
    "\n",
    "# 각 문장에 포함된 '단어의 평균 길이'\n",
    "train['mean_word_len']=train['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# 각 문장에 포함된 '겹치지 않는 단어의 개수'\n",
    "train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# 각 문장에 포함된 '문자의 개수'\n",
    "train[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "# 각 문장에 포함된 'stopwards(불용어)의 개수'\n",
    "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "# 각 문장에 포함된 '문장부호의 개수'\n",
    "train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# 각 문자에 포함된 단어 중 'Upper case로 된 단어의 비율'\n",
    "train[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))/train[\"num_words\"]\n",
    "\n",
    "# 각 문자에 포함된 단어 중 'title case(upper case + lower case)로 된 단어의 비율'\n",
    "train[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))/train[\"num_words\"]\n",
    "\n",
    "# 각 문장에 포함된 전체 문자의 개수 중 '','로 구분되어진 chunk에 포함된 문자들의 평균 개수에 대한 비율'\n",
    "train[\"chars_between_comma\"] = train[\"text\"].apply(lambda x: np.mean([len(chunk) for chunk in str(x).split(\",\")]))/train[\"num_chars\"]\n",
    "\n",
    "# 각 문장에 포함된 전체 문자의 개수 중 'ascii 문자나 숫자와 같은 symbol의 비율'\n",
    "train[\"symbols_unknowns\"]=train[\"text\"].apply(lambda x: np.sum([not w in symbols_knowns for w in str(x)]))/train[\"num_chars\"]\n",
    "\n",
    "# 각 문장에 포함된 '명사의 개수'\n",
    "train['noun'] = train[\"text\"].apply(lambda x: fraction_noun(x))\n",
    "\n",
    "# 각 문장에 포함된 '형용사의 개수'\n",
    "train['adj'] = train[\"text\"].apply(lambda x: fraction_adj(x))\n",
    "\n",
    "# 각 문장에 포함된 '동사의 개수'\n",
    "train['verbs'] = train[\"text\"].apply(lambda x: fraction_verbs(x))\n",
    "\n",
    "# 각 문장의 'SentimentIntensityAnalyzer의 compound 분석 값'\n",
    "train[\"sentiment\"]=train[\"text\"].apply(sentiment_nltk)\n",
    "\n",
    "# 각 문장에서 '단수 주어/주어/목적어 token이 포함된 갯수'\n",
    "train['single_frac'] = train['text'].apply(lambda x: count_tokens(x, ['is', 'was', 'has', 'he', 'she', 'it', 'her', 'his']))/train[\"num_words\"]\n",
    "\n",
    "# 각 문장에서 '복수 주어/주어/목적어 token이 포함된 갯수'\n",
    "train['plural_frac'] = train['text'].apply(lambda x: count_tokens(x, ['are', 'were', 'have', 'we', 'they']))/train[\"num_words\"]\n",
    "\n",
    "# 각 문장에 포함된 전체 문자의 개수에 대한 '첫번째 문자 길이의 비율'\n",
    "train['first_word_len']=train['text'].apply(first_word_len)/train[\"num_chars\"]\n",
    "\n",
    "# 각 문장에 포함된 전체 문자의 개수에 대한 '마지막 문자 길이의 비율'\n",
    "train['last_word_len']=train['text'].apply(last_word_len)/train[\"num_chars\"]\n",
    "\n",
    "# 첫번째 단어의 'symbol id를 구함'\n",
    "train[\"first_word_id\"] = train['text'].apply(lambda x: symbol_id(list(x.strip())[0]))\n",
    "\n",
    "# 마지막 단어의 'symbol id를 구함'\n",
    "train[\"last_word_id\"] = train['text'].apply(lambda x: symbol_id(list(x.strip())[-1]))\n",
    "\n",
    "# flesch_reading_ease score를 계산\n",
    "train['ease']=train['text'].apply(flesch_reading_ease)\n",
    "\n",
    "\n",
    "# 동일 feature를 test data에 대해서도 생성\n",
    "test['num_words']=test['text'].apply(lambda x:len(str(x).split()))\n",
    "test['mean_word_len']=test['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\n",
    "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n",
    "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))/test[\"num_words\"]\n",
    "test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))/test[\"num_words\"]\n",
    "test[\"chars_between_comma\"] = test[\"text\"].apply(lambda x: np.mean([len(chunk) for chunk in str(x).split(\",\")]))/test[\"num_chars\"]\n",
    "test[\"symbols_unknowns\"]=test[\"text\"].apply(lambda x: np.sum([not w in symbols_knowns for w in str(x)]))/test[\"num_chars\"]\n",
    "test['noun'] = test[\"text\"].apply(lambda x: fraction_noun(x))\n",
    "test['adj'] = test[\"text\"].apply(lambda x: fraction_adj(x))\n",
    "test['verbs'] = test[\"text\"].apply(lambda x: fraction_verbs(x))\n",
    "test[\"sentiment\"]=test[\"text\"].apply(sentiment_nltk)\n",
    "test['single_frac'] = test['text'].apply(lambda x: count_tokens(x, ['is', 'was', 'has', 'he', 'she', 'it', 'her', 'his']))/test[\"num_words\"]\n",
    "test['plural_frac'] = test['text'].apply(lambda x: count_tokens(x, ['are', 'were', 'have', 'we', 'they']))/test[\"num_words\"]\n",
    "test['first_word_len']=test['text'].apply(first_word_len)/test[\"num_chars\"]\n",
    "test['last_word_len']=test['text'].apply(last_word_len)/test[\"num_chars\"]\n",
    "test[\"first_word_id\"] = test['text'].apply(lambda x: symbol_id(list(x.strip())[0]))\n",
    "test[\"last_word_id\"] = test['text'].apply(lambda x: symbol_id(list(x.strip())[-1]))\n",
    "test['ease']=test['text'].apply(flesch_reading_ease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-funeral",
   "metadata": {},
   "source": [
    "### 작가별 등장인물의 list를 만들어서 text에 등장하는 인물과의 유사도를 계산하는 듯!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "planned-vegetable",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다: 'C:\\\\Users\\\\marti/nltk_data'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-be86deb95b5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"persons_0\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpersons_author_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"persons_1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpersons_author_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"persons_2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpersons_author_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"persons_3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpersons_author_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"persons_4\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpersons_author_4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4136\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4138\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-be86deb95b5d>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"persons_0\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpersons_author_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"persons_1\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpersons_author_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"persons_2\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpersons_author_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"persons_3\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpersons_author_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"persons_4\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mjaccard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpersons_author_4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-be86deb95b5d>\u001b[0m in \u001b[0;36mget_persons\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mres_ne_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 품사를 태깅한 token을 개체명 인식하여 tree형태로 반환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mres_ne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_ne_tree\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# ne_chunk로 인해 tree로 반환된 것을 IOB 형태로 변환하여 줌.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mres_ne_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres_ne\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# IOB 형태가 된 token을 list 형태로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \"\"\"\n\u001b[1;32m--> 160\u001b[1;33m     \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mtagger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m             )\n\u001b[0;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    522\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[1;31m# Is the path item a zipfile?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 524\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mpath_\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mpath_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".zip\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    525\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mZipFilePathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_persons(text):\n",
    "    def bind_names(tagged_words):\n",
    "        names=list()\n",
    "        name=list()\n",
    "        for i,w in enumerate(tagged_words): # 반복문 사용 시 몇 번째 반복문인지 확인 / 인덱스 번호와 컬렉션의 원소를 tuple형태로 반환 \n",
    "            if(\"PERSON\" in w[2]):\n",
    "                name.append(w[0])\n",
    "            else:\n",
    "                if(len(name)!=0):\n",
    "                    names.append(\" \".join(name))\n",
    "                name=list()\n",
    "                \n",
    "            if(i==len(tagged_words)-1 and len(name)!=0):\n",
    "                names.append(\" \".join(name))\n",
    "        return names                   \n",
    "\n",
    "    res_ne_tree = ne_chunk(pos_tag(word_tokenize(text))) # 품사를 태깅한 token을 개체명 인식하여 tree형태로 반환\n",
    "    res_ne = tree2conlltags(res_ne_tree) # ne_chunk로 인해 tree로 반환된 것을 IOB 형태로 변환하여 줌.\n",
    "    res_ne_list = [list(x) for x in res_ne] # IOB 형태가 된 token을 list 형태로 변환\n",
    "    return bind_names(res_ne_list)\n",
    "\n",
    "\n",
    "text_author_0 = \" \".join(list(train['text'][train['author']==0]))\n",
    "text_author_1 = \" \".join(list(train['text'][train['author']==1]))\n",
    "text_author_2 = \" \".join(list(train['text'][train['author']==2]))\n",
    "text_author_3 = \" \".join(list(train['text'][train['author']==3]))\n",
    "text_author_4 = \" \".join(list(train['text'][train['author']==4]))\n",
    "\n",
    "persons_author_0 = set(get_persons(text_author_0))\n",
    "persons_author_1 = set(get_persons(text_author_1))\n",
    "persons_author_2 = set(get_persons(text_author_2))\n",
    "persons_author_3 = set(get_persons(text_author_3))\n",
    "persons_author_4 = set(get_persons(text_author_4))\n",
    "\n",
    "# 자카드 지수(Jaccard index)는 두 집합 사이의 유사도를 측정하는 방법 중 하나이다. \n",
    "# 자카드 계수(Jaccard coefficient) 또는 자카드 유사도(Jaccard similarity)라고도 한다. \n",
    "# 자카드 지수는 0과 1 사이의 값을 가지며, 두 집합이 동일하면 1의 값을 가지고, 공통의 원소가 하나도 없으면 0의 값을 가진다.\n",
    "def jaccard(a,b):\n",
    "    return len(a&b)/len(a|b)\n",
    "\n",
    "# 작가별 등장 인물의 유사도를 feature로 넣는듯!!\n",
    "\n",
    "train[\"persons_0\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_0)) \n",
    "train[\"persons_1\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_1)) \n",
    "train[\"persons_2\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_2)) \n",
    "train[\"persons_3\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_3)) \n",
    "train[\"persons_4\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_4)) \n",
    "\n",
    "test[\"persons_0\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_0)) \n",
    "test[\"persons_1\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_1)) \n",
    "test[\"persons_2\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_2)) \n",
    "test[\"persons_3\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_3)) \n",
    "test[\"persons_4\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-economics",
   "metadata": {},
   "source": [
    "## fasttext\n",
    "\n",
    "### festtext.train_unsupervised('data.txt')\n",
    "\n",
    "- 파라미터로 모델 지정이 가능함.('skipgram', 'cbow')\n",
    "- prameters\n",
    "    - input : training file path **(required)**\n",
    "    - model : unsupervised fasttext model / {cbow, skipgram} / default=skipgram\n",
    "    - lr : learning rate / default=0.05\n",
    "    - dim : size of word vectors / default=100\n",
    "    - ws : size of the context window / default=5\n",
    "    - epoch : number of epochs / default=5\n",
    "    - minCount : minimal number of word occurences / default=5\n",
    "    - minn : min length of char ngram / default=3\n",
    "    - maxn : min length of char ngram / default=6\n",
    "    - neg : number of negatives sampled / default=5\n",
    "    - wordNgrams : max length of wrd ngram / default=1\n",
    "    - loss : loss fuction / {ns, hs, softmax, ova} / default=ns\n",
    "    - bucket : number of buckets / default=2,000,000\n",
    "    - thread : number of treads / default=number of cpus\n",
    "    - lrUpdateRate : change the rate of updates for the learning rate / default=100\n",
    "    - t : sampling threshold / default=0.0001\n",
    "    - verbose : verbose / default=2\n",
    "- Model object fuctions\n",
    "    - get_dimension           \n",
    "        - Get the dimension (size) of a lookup vector (hidden layer).\n",
    "        - This is equivalent to `dim` property.\n",
    "    - get_input_vector        \n",
    "        - Given an index, get the corresponding vector of the Input Matrix.\n",
    "    - get_input_matrix        \n",
    "        - Get a copy of the full input matrix of a Model.\n",
    "    - get_labels              \n",
    "        - Get the entire list of labels of the dictionary\n",
    "        - This is equivalent to `labels` property.\n",
    "    - get_line                \n",
    "        - Split a line of text into words and labels.\n",
    "    - get_output_matrix       \n",
    "        - Get a copy of the full output matrix of a Model.\n",
    "    - get_sentence_vector     \n",
    "        - Given a string, get a single vector represenation. This function\n",
    "        - assumes to be given a single line of text. We split words on\n",
    "        - whitespace (space, newline, tab, vertical tab) and the control\n",
    "        - characters carriage return, formfeed and the null character.\n",
    "    - get_subword_id          \n",
    "        - Given a subword, return the index (within input matrix) it hashes to.\n",
    "    - get_subwords            \n",
    "        - Given a word, get the subwords and their indicies.\n",
    "    - get_word_id             \n",
    "        - Given a word, get the word id within the dictionary.\n",
    "    - get_word_vector         \n",
    "        - Get the vector representation of word.\n",
    "    - get_words               \n",
    "        - Get the entire list of words of the dictionary\n",
    "        - This is equivalent to `words` property.\n",
    "    - is_quantized            \n",
    "        - whether the model has been quantized\n",
    "    - predict                 \n",
    "        - Given a string, get a list of labels and a list of corresponding probabilities.\n",
    "    - quantize                \n",
    "        - Quantize the model reducing the size of the model and it's memory footprint.\n",
    "    - save_model              \n",
    "        - Save the model to the given path\n",
    "    - test                    \n",
    "        - Evaluate supervised model using file given by path\n",
    "    - test_label              \n",
    "        - Return the precision and recall score for each label.\n",
    "        \n",
    "### 실습\n",
    "~~~python\n",
    "import fasttext\n",
    "model = fasttext.train_unsupervised('review.sorted.uniq.refined.tsv.text.tok',model='skipgram', epoch=5,lr = 0.1)\n",
    "\n",
    "print(model['행사']) # get the vector of the word '행사'\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'].to_csv(data_path + 'sample_file.txt',index=False, header=None, sep=\"\\t\")\n",
    "model_ft = fasttext.train_unsupervised(data_path + 'sample_file.txt', minCount=2, minn=2, maxn=10,dim=300)\n",
    "\n",
    "def sent2vec(s):\n",
    "    words = nltk.tokenize.word_tokenize(s)\n",
    "    #words = [k.stem(w) for w in words]\n",
    "    #words = [w for w in words if not w in string.digits]\n",
    "    #words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model_ft[w]) # 해당 단어에 대한 vector를 추정.\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M) # vector를 array에 append\n",
    "    v = M.sum(axis=0) # axis=0을 기준으로 vector값을 더함. ==> 왜 이런 연산이 필요하지..?(하나의 vector로 만들어주려고..?)\n",
    "    if type(v) != np.ndarray: # 만들어진 해당 vector가 numpy의 array가 아닐 경우\n",
    "        return np.zeros(300) # 임의의 0으로 채워진 numpy array 반환\n",
    "    return v\n",
    "\n",
    "# train과 test의 각각 text data를 vector로 만들기\n",
    "xtrain_ft = np.array([sent2vec(x) for x in train['text']])\n",
    "xtest_ft = np.array([sent2vec(x) for x in test['text']])\n",
    "\n",
    "# 반환된 numpy arrya를 pandas dataframe으로 바꾸어주기\n",
    "train_ft=pd.DataFrame(xtrain_ft)\n",
    "# columns 명(feature명) 바꾸어 주기\n",
    "train_ft.columns = ['ft_vector_'+str(i) for i in range(xtrain_ft.shape[1])]\n",
    "\n",
    "# 반환된 numpy arrya를 pandas dataframe으로 바꾸어주기\n",
    "test_ft=pd.DataFrame(xtest_ft)\n",
    "# columns 명(feature명) 바꾸어 주기\n",
    "test_ft.columns = ['ft_vector_'+str(i) for i in range(xtrain_ft.shape[1])]\n",
    "\n",
    "# 기존의 data와 연결함.\n",
    "train = pd.concat([train, train_ft], axis=1)\n",
    "test = pd.concat([test, test_ft], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf20756",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-0ebcd77e81d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# TfidfVectorizer를 정의\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtfidf_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# 값을 list로 변환하여 전달\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrain_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "# TfidfVectorizer -> 문서목록에서 각 문서의 feature를 tf-idf 값을 가중치로 설정한 BOW 벡터를 만든다.\n",
    "# TF-IDF : TF-IDF(Term Frequency – Inverse Document Frequency)는 정보검색론(IR-Information Retrieval) 에선 흔하게 접하는 가중치를 구하는 알고리즘 입니다.\n",
    "\n",
    "# TfidfVectorizer를 정의\n",
    "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "# 값을 list로 변환하여 전달\n",
    "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
    "# train data로 fit되었으므로 그냥 transform, 마찬가지로 list로 변환하여 전달\n",
    "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
    "# label 값 분할\n",
    "train_y = train['author']\n",
    "\n",
    "# LogisticRegression의 ingerense 과정 정의\n",
    "def runLR(train_X,train_y,test_X,test_y,test_X2):\n",
    "    model=LogisticRegression()\n",
    "    model.fit(train_X,train_y)\n",
    "    pred_test_y=model.predict_proba(test_X)\n",
    "    pred_test_y2=model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "\n",
    "cv_scores=[]\n",
    "\n",
    "# training에 필요없는 feature drop\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "# label data 분할\n",
    "train_y=train['author']\n",
    "\n",
    "# inference에 필요없는 feature drop\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y): # 나눈 fold의 index를 반환!\n",
    "    # fold 별로 분할\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "    # dev fold에 대해 inference\n",
    "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
    "    \n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    \n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "    \n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "train[\"tfidf_LR_0\"] = pred_train[:,0]\n",
    "train[\"tfidf_LR_1\"] = pred_train[:,1]\n",
    "train[\"tfidf_LR_2\"] = pred_train[:,2]\n",
    "train[\"tfidf_LR_3\"] = pred_train[:,3]\n",
    "train[\"tfidf_LR_4\"] = pred_train[:,4]\n",
    "test[\"tfidf_LR_0\"] = pred_full_test[:,0]\n",
    "test[\"tfidf_LR_1\"] = pred_full_test[:,1]\n",
    "test[\"tfidf_LR_2\"] = pred_full_test[:,2]\n",
    "test[\"tfidf_LR_3\"] = pred_full_test[:,3]\n",
    "test[\"tfidf_LR_4\"] = pred_full_test[:,4]\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "cvec_vec=CountVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "cvec_vec.fit(train['text'].values.tolist())\n",
    "train_cvec = cvec_vec.transform(train['text'].values.tolist())\n",
    "test_cvec = cvec_vec.transform(test['text'].values.tolist())\n",
    "\n",
    "cv_scores=[]\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "train_y=train['author']\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_cvec[dev_index], train_cvec[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_cvec)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "train[\"cvec_LR_0\"] = pred_train[:,0]\n",
    "train[\"cvec_LR_1\"] = pred_train[:,1]\n",
    "train[\"cvec_LR_2\"] = pred_train[:,2]\n",
    "train[\"cvec_LR_3\"] = pred_train[:,3]\n",
    "train[\"cvec_LR_4\"] = pred_train[:,4]\n",
    "test[\"cvec_LR_0\"] = pred_full_test[:,0]\n",
    "test[\"cvec_LR_1\"] = pred_full_test[:,1]\n",
    "test[\"cvec_LR_2\"] = pred_full_test[:,2]\n",
    "test[\"cvec_LR_3\"] = pred_full_test[:,3]\n",
    "test[\"cvec_LR_4\"] = pred_full_test[:,4]\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
    "cvec_char_vec.fit(train['text'].values.tolist())\n",
    "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
    "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
    "\n",
    "cv_scores=[]\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "train_y=train['author']\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_cvec_char)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "train[\"cvec_char_LR_0\"] = pred_train[:,0]\n",
    "train[\"cvec_char_LR_1\"] = pred_train[:,1]\n",
    "train[\"cvec_char_LR_2\"] = pred_train[:,2]\n",
    "train[\"cvec_char_LR_3\"] = pred_train[:,3]\n",
    "train[\"cvec_char_LR_4\"] = pred_train[:,4]\n",
    "test[\"cvec_char_LR_0\"] = pred_full_test[:,0]\n",
    "test[\"cvec_char_LR_1\"] = pred_full_test[:,1]\n",
    "test[\"cvec_char_LR_2\"] = pred_full_test[:,2]\n",
    "test[\"cvec_char_LR_3\"] = pred_full_test[:,3]\n",
    "test[\"cvec_char_LR_4\"] = pred_full_test[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac1f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
