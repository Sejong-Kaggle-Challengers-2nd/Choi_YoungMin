{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intellectual-procurement",
   "metadata": {},
   "source": [
    "# 사용된 XGBoost Feature\n",
    "\n",
    "- Meta Feature (문장 길이, Stop words 갯수, ..., Named Entity)\n",
    "- FastText Embedding\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- SGDClassifier\n",
    "- RandomForestClassifier\n",
    "- MLPClassifier\n",
    "- DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "documented-triangle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "data_path = '../../kaggle_data/fiction_author/'\n",
    "\n",
    "train = pd.read_csv(data_path + 'train.csv', encoding='utf-8')\n",
    "test = pd.read_csv(data_path + 'test_x.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suburban-substitute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>He was almost choking. There was so much, so m...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>“Your sister asked for it, I suppose?”</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>She was engaged one day as she walked, in per...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The captain was in the porch, keeping himself ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>“Have mercy, gentlemen!” odin flung up his han...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text  author\n",
       "0      0  He was almost choking. There was so much, so m...       3\n",
       "1      1             “Your sister asked for it, I suppose?”       2\n",
       "2      2   She was engaged one day as she walked, in per...       1\n",
       "3      3  The captain was in the porch, keeping himself ...       4\n",
       "4      4  “Have mercy, gentlemen!” odin flung up his han...       3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bridal-danish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>“Not at all. I think she is one of the most ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"No,\" replied he, with sudden consciousness, \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>As the lady had stated her intention of scream...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>“And then suddenly in the silence I heard a so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>His conviction remained unchanged. So far as I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               text\n",
       "0      0  “Not at all. I think she is one of the most ch...\n",
       "1      1  \"No,\" replied he, with sudden consciousness, \"...\n",
       "2      2  As the lady had stated her intention of scream...\n",
       "3      3  “And then suddenly in the silence I heard a so...\n",
       "4      4  His conviction remained unchanged. So far as I..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-pride",
   "metadata": {},
   "source": [
    "### textstat\n",
    "\n",
    "- Textstat는 텍스트에서 통계를 계산하는 데 사용하기 쉬운 라이브러리입니다. 가독성, 복잡성 및 등급 수준을 결정하는 데 도움이 됩니다.\n",
    "\n",
    "### fasttext\n",
    "\n",
    "- 단어를 벡터로 만드는 또 다른 방법으로는 페이스북에서 개발한 FastText가 있습니다.\n",
    "- Word2Vec 이후에 나온 것이기 때문에, 메커니즘 자체는 Word2Vec의 확장이라고 볼 수 있습니다. \n",
    "- Word2Vec와 FastText와의 가장 큰 차이점이라면 Word2Vec는 단어를 쪼개질 수 없는 단위로 생각한다면, FastText는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주합니다. 즉 내부 단어(subword)를 고려하여 학습합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "monetary-mexico",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textstat in c:\\users\\marti\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: pyphen in c:\\users\\marti\\anaconda3\\lib\\site-packages (from textstat) (0.10.0)\n",
      "Requirement already satisfied: fasttext in c:\\users\\marti\\anaconda3\\lib\\site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\marti\\anaconda3\\lib\\site-packages (from fasttext) (2.6.2)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\marti\\anaconda3\\lib\\site-packages (from fasttext) (52.0.0.post20210125)\n",
      "Requirement already satisfied: numpy in c:\\users\\marti\\anaconda3\\lib\\site-packages (from fasttext) (1.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat\n",
    "!pip install fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-cleveland",
   "metadata": {},
   "source": [
    "## 1) flesch_reading_ease\n",
    "\n",
    "- Flesch reading-ease test 에서는 점수가 높을수록 읽기 쉬운 재료를 나타내며 숫자가 낮을수록 읽기 어려운 구절을 표시합니다.\n",
    "- Flesch Reading Ease Score를 반환합니다.\n",
    "- 최대 점수는 121.22점이지만 점수가 얼마나 낮을 수 있는지에 대한 제한은 없습니다. 음수 점수가 유효합니다.\n",
    "\n",
    "## 2) nltk\n",
    "\n",
    "- 교육용으로 개발된 자연어 처리 및 문서 분석용 파이썬 패키지. 다양한 기능 및 예제를 가지고 있으며 실무 및 연구에서도 많이 사용됩니다.\n",
    "- 말뭉치(corpus)는 자연어 분석 작업을 위해 만든 샘플 문서 집합을 말한다. 단순히 소설, 신문 등의 문서를 모아놓은 것도 있지만 품사. 형태소, 등의 보조적 의미를 추가하고 쉬운 분석을 위해 구조적인 형태로 정리해 놓은 것을 포함한다.\n",
    "- 말뭉치 자료는 설치시에 제공되지 않고 download 명령으로 사용자가 다운로드 받아야 한다.\n",
    "- nltk.download(\"book\") 명령을 실행하면 NLTK 패키지 사용자 설명서에서 요구하는 대부분의 말뭉치를 다운로드 받아준다.\n",
    "\n",
    "### 2-1) nltk.tokenize\n",
    "\n",
    "- 자연어 문서를 분석하기 위해서는 우선 긴 문자열을 분석을 위한 작은 단위로 나누어야 한다. 이 문자열 단위를 토큰(token)이라고 하고 이렇게 문자열을 토큰으로 나누는 작업을 토큰 생성(tokenizing)이라고 한다. 영문의 경우에는 문장, 단어 등을 토큰으로 사용하거나 정규 표현식을 쓸 수 있다.\n",
    "- 문자열을 토큰으로 분리하는 함수를 토큰 생성 함수(tokenizer)라고 한다. 토큰 생성 함수는 문자열을 입력받아 토큰 문자열의 리스트를 출력한다.\n",
    "~~~python\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(emma_raw[50:100])\n",
    "~~~\n",
    "> ['Emma',\n",
    " 'Woodhouse',\n",
    " ',',\n",
    " 'handsome',\n",
    " ',',\n",
    " 'clever',\n",
    " ',',\n",
    " 'and',\n",
    " 'rich',\n",
    " ',',\n",
    " 'with',\n",
    " 'a']\n",
    " \n",
    "### 2-2) from nltk.tag import pos_tag (품사 부착)\n",
    " \n",
    " - 품사(POS, part-of-speech)는 낱말을 문법적인 기능이나 형태, 뜻에 따라 구분한 것이다. 품사의 구분은 언어마다 그리고 학자마다 다르다. 예를 들어 NLTK에서는 펜 트리뱅크 태그세트(Penn Treebank Tagset)라는 것을 이용한다. 다음은 펜 트리뱅크 태그세트에서 사용하는 품사의 예이다.\n",
    "     - NNP : 단수 고유명사\n",
    "     - VB : 동사\n",
    "     - VBP : 동사 현재형\n",
    "     - TO : to 전칳사\n",
    "     - NN : 명사(단수형 혹은 집합형)\n",
    "     - DT : 관형사\n",
    " - pos_tag 명령을 사용하면 단어 토큰에 품사를 부착하여 튜플로 출력한다. 다음 예문에서 refuse, permit이라는 같은 철자의 단어가 각각 동사와 명사로 다르게 품사 부착된 것을 볼 수 있다.\n",
    " ~~~python\n",
    "from nltk.tag import pos_tag\n",
    "sentence = \"Emma refused to permit us to obtain the refuse permit\"\n",
    "tagged_list = pos_tag(word_tokenize(sentence))\n",
    "tagged_list\n",
    "~~~\n",
    "> [('Emma', 'NNP'),\n",
    " ('refused', 'VBD'),\n",
    " ('to', 'TO'),\n",
    " ('permit', 'VB'),\n",
    " ('us', 'PRP'),\n",
    " ('to', 'TO'),\n",
    " ('obtain', 'VB'),\n",
    " ('the', 'DT'),\n",
    " ('refuse', 'NN'),\n",
    " ('permit', 'NN')]\n",
    " \n",
    " - Scikit-Learn 등에서 자연어 분석을 할 때는 같은 토큰이라도 품사가 다르면 다른 토큰으로 처리해야 하는 경우가 많은데 이 때는 원래의 토큰과 품사를 붙여서 새로운 토큰 이름을 만들어 사용하면 철자가 같고 품사가 다른 단어를 구분할 수 있다.\n",
    " \n",
    "### 2-3) nltk.ne_chunk\n",
    "\n",
    "- nltk 라이브러리 ne_chunk() 함수를 사용해서 개체명을 인식시킬 수 있다\n",
    "- 개체명 인식을 사용하면 코퍼스로부터 어떤 단어가 사람, 장소, 조직 등을 의미하는 단어인지를 찾을 수 있습니다.\n",
    "- 어떤 이름을 의미하는 단어를 보고는 그 단어가 어떤 유형인지를 인식하는 것을 말합니다.\n",
    "    - \"유정이는 2018년에 골드만삭스에 입사했다.\"\n",
    "    - 유정 -> 사람 / 2018년 -> 시간 / 골드만삭스 -> 조직\n",
    "- NLTK에서는 개체명 인식기(NER chunker)를 지원하고 있으므로, 별도 개체명 인식기를 구현할 필요없이 NLTK를 사용해서 개체명 인식을 수행할 수 있습니다.\n",
    "- ne_chunk는 개체명을 태깅하기 위해서 앞서 품사 태깅(pos_tag)이 수행되어야 합니다. 위의 결과에서 James는 PERSON(사람), Disney는 조직(ORGANIZATION), London은 위치(GPE)라고 정상적으로 개체명 인식이 수행된 것을 볼 수 있습니다.\n",
    "~~~python\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "sentence = \"James is working at Disney in London\"\n",
    "sentence=pos_tag(word_tokenize(sentence))\n",
    "print(sentence) # 토큰화와 품사 태깅을 동시 수행\n",
    "~~~\n",
    "> [('James', 'NNP'), ('is', 'VBZ'), ('working', 'VBG'), ('at', 'IN'), ('Disney', 'NNP'), ('in', 'IN'), ('London', 'NNP')]\n",
    "~~~python\n",
    "sentence=ne_chunk(sentence)\n",
    "print(sentence) # 개체명 인식\n",
    "~~~\n",
    "> (S\n",
    "  (PERSON James/NNP)\n",
    "  is/VBZ\n",
    "  working/VBG\n",
    "  at/IN\n",
    "  (ORGANIZATION Disney/NNP)\n",
    "  in/IN\n",
    "  (GPE London/NNP))\n",
    "\n",
    "### 2-4) from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "- 문자열을 가져와서 네 가지 범주 각각에 대한 점수 dictionary를 반환합니다.\n",
    "    - negative\n",
    "    - neutral\n",
    "    - positive\n",
    "    - compound(computed by normalizing the scores above)\n",
    "    \n",
    "> a = 'This was a good movie.'\n",
    "> sid.polarity_scores(a)\n",
    "\n",
    "> OUTPUT-{'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\n",
    "\n",
    "> a = 'This was the best, most awesome movie EVER MADE!!!'\n",
    "> sid.polarity_scores(a)\n",
    "\n",
    "> OUTPUT-{'neg': 0.0, 'neu': 0.425, 'pos': 0.575, 'compound': 0.8877}\n",
    "\n",
    "### 2-5) from nltk.corpus import stopwords\n",
    "\n",
    "- 갖고 있는 데이터에서 유의미한 단어 토큰만을 선별하기 위해서는 큰 의미가 없는 단어 토큰을 제거하는 작업이 필요합니다. \n",
    "- 여기서 큰 의미가 없다라는 것은 자주 등장하지만 분석을 하는 것에 있어서는 큰 도움이 되지 않는 단어들을 말합니다. \n",
    "- 예를 들면, I, my, me, over, 조사, 접미사 같은 단어들은 문장에서는 자주 등장하지만 실제 의미 분석을 하는데는 거의 기여하는 바가 없는 경우가 있습니다. \n",
    "- 이러한 단어들을 불용어(stopword)라고 하며, NLTK에서는 위와 같은 100여개 이상의 영어 단어들을 불용어로 패키지 내에서 미리 정의하고 있습니다.\n",
    "~~~python\n",
    "from nltk.corpus import stopwords  \n",
    "stopwords.words('english')[:10]\n",
    "~~~\n",
    "> ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dominant-description",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textstat import flesch_reading_ease\n",
    "\n",
    "import fasttext\n",
    "\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk, tree2conlltags\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import string\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn import metrics, model_selection, naive_bayes\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "symbols_knowns = string.ascii_letters + string.digits + string.punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-field",
   "metadata": {},
   "source": [
    "- punctuation ; 따옴표, 마침표, 물음표 등등 과 같은 문장부호"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "prostate-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_nltk(text):\n",
    "    res = SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "    return res['compound']\n",
    "\n",
    "def get_words(text):\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    return [word for word in words if not word in string.punctuation]\n",
    "    \n",
    "def count_tokens(text, tokens):\n",
    "    return sum([w in tokens for w in get_words(text)])\n",
    "\n",
    "def first_word_len(text):\n",
    "    if(len(get_words(text))==0):\n",
    "        return 0\n",
    "    else:   \n",
    "        return len(get_words(text)[0])\n",
    "\n",
    "def last_word_len(text):\n",
    "    if(len(get_words(text))==0):\n",
    "        return 0\n",
    "    else:   \n",
    "        return len(get_words(text)[-1])\n",
    "\n",
    "def symbol_id(x):\n",
    "    symbols=[x for x in symbols_knowns]\n",
    "      \n",
    "    if x not in symbols:\n",
    "        return -1 \n",
    "    else:\n",
    "        return np.where(np.array(symbols) == x )[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cardiovascular-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 전체 단어 개수에서 '명사'의 품사를 가진 단어의 비율을 구함.\n",
    "def fraction_noun(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__() # len()과 object.__len__()은 다름. len()은 __len__()을 호출하는 것임.\n",
    "    if word_count==0:\n",
    "        return 0\n",
    "    else:\n",
    "        pos_list = nltk.pos_tag(text_splited)\n",
    "        noun_count = len([w for w in pos_list if w[1] in ('NN','NNP','NNPS','NNS')])\n",
    "    \n",
    "        return (noun_count/word_count)\n",
    "    \n",
    "## 전체 단어 개수에서 '형용사'의 품사를 가진 단어의 비율을 구함.\n",
    "def fraction_adj(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    if word_count==0:\n",
    "        return 0\n",
    "    else:\n",
    "        pos_list = nltk.pos_tag(text_splited)\n",
    "        adj_count = len([w for w in pos_list if w[1] in ('JJ','JJR','JJS')])\n",
    "    \n",
    "        return (adj_count/word_count)  \n",
    "\n",
    "## 전체 단어 개수에서 '동사'의 품사를 가진 단어의 비율을 구함.\n",
    "def fraction_verbs(text):\n",
    "    text_splited = text.split(' ')\n",
    "    text_splited = [''.join(c for c in s if c not in string.punctuation) for s in text_splited]\n",
    "    text_splited = [s for s in text_splited if s]\n",
    "    word_count = text_splited.__len__()\n",
    "    if word_count==0:\n",
    "        return 0\n",
    "    else:\n",
    "        pos_list = nltk.pos_tag(text_splited)\n",
    "        verbs_count = len([w for w in pos_list if w[1] in ('VB','VBD','VBG','VBN','VBP','VBZ')])\n",
    "    \n",
    "        return (verbs_count/word_count)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-engineering",
   "metadata": {},
   "source": [
    "## 생성한 feature\n",
    "\n",
    "1. 각 문장에 포함된 **'단어의 개수'**\n",
    "2. 각 문장에 포함된 **'단어의 평균 길이'**\n",
    "3. 각 문장에 포함된 **'겹치지 않는 단어의 개수'**\n",
    "4. 각 문장에 포함된 **'문자의 개수'**\n",
    "5. 각 문장에 포함된 **'stopwards(불용어)의 개수'**\n",
    "6. 각 문장에 포함된 **'문장부호의 개수'**\n",
    "7. 각 문자에 포함된 단어 중 **'Upper case로 된 단어의 비율'**\n",
    "8. 각 문자에 포함된 단어 중 **'title case(upper case + lower case)로 된 단어의 비율'**\n",
    "9. 각 문장에 포함된 전체 문자의 개수 중 **'','로 구분되어진 chunk에 포함된 문자들의 평균 개수에 대한 비율'**\n",
    "10. 각 문장에 포함된 전체 문자의 개수 중 **'ascii 문자나 숫자와 같은 symbol의 비율'**\n",
    "11. 각 문장에 포함된 **'명사의 개수'**\n",
    "12. 각 문장에 포함된 **'형용사의 개수'**\n",
    "13. 각 문장에 포함된 **'동사의 개수'**\n",
    "14. 각 문장의 **'SentimentIntensityAnalyzer의 compound 분석 값'**\n",
    "15. 각 문장에서 **'단수 주어/주어/목적어 token이 포함된 갯수'**\n",
    "16. 각 문장에서 **'복수 주어/주어/목적어 token이 포함된 갯수'**\n",
    "17. 각 문장에 포함된 전체 문자의 개수에 대한 **'첫번째 문자 길이의 비율'**\n",
    "18. 각 문장에 포함된 전체 문자의 개수에 대한 **'마지막 문자 길이의 비율'**\n",
    "19. **첫번째 단어의 'symbol id를 구함'**\n",
    "20. **마지막 단어의 'symbol id를 구함'**\n",
    "21. **flesch_reading_ease score**를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "twenty-jurisdiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 문장에 포함된 '단어의 개수'\n",
    "train['num_words']=train['text'].apply(lambda x:len(get_words(x)))\n",
    "\n",
    "# 각 문장에 포함된 '단어의 평균 길이'\n",
    "train['mean_word_len']=train['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "# 각 문장에 포함된 '겹치지 않는 단어의 개수'\n",
    "train[\"num_unique_words\"] = train[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "# 각 문장에 포함된 '문자의 개수'\n",
    "train[\"num_chars\"] = train[\"text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "# 각 문장에 포함된 'stopwards(불용어)의 개수'\n",
    "train[\"num_stopwords\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "# 각 문장에 포함된 '문장부호의 개수'\n",
    "train[\"num_punctuations\"] =train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "\n",
    "# 각 문자에 포함된 단어 중 'Upper case로 된 단어의 비율'\n",
    "train[\"num_words_upper\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))/train[\"num_words\"]\n",
    "\n",
    "# 각 문자에 포함된 단어 중 'title case(upper case + lower case)로 된 단어의 비율'\n",
    "train[\"num_words_title\"] = train[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))/train[\"num_words\"]\n",
    "\n",
    "# 각 문장에 포함된 전체 문자의 개수 중 '','로 구분되어진 chunk에 포함된 문자들의 평균 개수에 대한 비율'\n",
    "train[\"chars_between_comma\"] = train[\"text\"].apply(lambda x: np.mean([len(chunk) for chunk in str(x).split(\",\")]))/train[\"num_chars\"]\n",
    "\n",
    "# 각 문장에 포함된 전체 문자의 개수 중 'ascii 문자나 숫자와 같은 symbol의 비율'\n",
    "train[\"symbols_unknowns\"]=train[\"text\"].apply(lambda x: np.sum([not w in symbols_knowns for w in str(x)]))/train[\"num_chars\"]\n",
    "\n",
    "# 각 문장에 포함된 '명사의 개수'\n",
    "train['noun'] = train[\"text\"].apply(lambda x: fraction_noun(x))\n",
    "\n",
    "# 각 문장에 포함된 '형용사의 개수'\n",
    "train['adj'] = train[\"text\"].apply(lambda x: fraction_adj(x))\n",
    "\n",
    "# 각 문장에 포함된 '동사의 개수'\n",
    "train['verbs'] = train[\"text\"].apply(lambda x: fraction_verbs(x))\n",
    "\n",
    "# 각 문장의 'SentimentIntensityAnalyzer의 compound 분석 값'\n",
    "train[\"sentiment\"]=train[\"text\"].apply(sentiment_nltk)\n",
    "\n",
    "# 각 문장에서 '단수 주어/주어/목적어 token이 포함된 갯수'\n",
    "train['single_frac'] = train['text'].apply(lambda x: count_tokens(x, ['is', 'was', 'has', 'he', 'she', 'it', 'her', 'his']))/train[\"num_words\"]\n",
    "\n",
    "# 각 문장에서 '복수 주어/주어/목적어 token이 포함된 갯수'\n",
    "train['plural_frac'] = train['text'].apply(lambda x: count_tokens(x, ['are', 'were', 'have', 'we', 'they']))/train[\"num_words\"]\n",
    "\n",
    "# 각 문장에 포함된 전체 문자의 개수에 대한 '첫번째 문자 길이의 비율'\n",
    "train['first_word_len']=train['text'].apply(first_word_len)/train[\"num_chars\"]\n",
    "\n",
    "# 각 문장에 포함된 전체 문자의 개수에 대한 '마지막 문자 길이의 비율'\n",
    "train['last_word_len']=train['text'].apply(last_word_len)/train[\"num_chars\"]\n",
    "\n",
    "# 첫번째 단어의 'symbol id를 구함'\n",
    "train[\"first_word_id\"] = train['text'].apply(lambda x: symbol_id(list(x.strip())[0]))\n",
    "\n",
    "# 마지막 단어의 'symbol id를 구함'\n",
    "train[\"last_word_id\"] = train['text'].apply(lambda x: symbol_id(list(x.strip())[-1]))\n",
    "\n",
    "# flesch_reading_ease score를 계산\n",
    "train['ease']=train['text'].apply(flesch_reading_ease)\n",
    "\n",
    "\n",
    "# 동일 feature를 test data에 대해서도 생성\n",
    "test['num_words']=test['text'].apply(lambda x:len(str(x).split()))\n",
    "test['mean_word_len']=test['text'].apply(lambda x:np.mean([len(w) for w in str(x).split()]))\n",
    "test[\"num_unique_words\"] = test[\"text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test[\"num_chars\"] = test[\"text\"].apply(lambda x: len(str(x)))\n",
    "test[\"num_stopwords\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test[\"num_punctuations\"] =test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test[\"num_words_upper\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))/test[\"num_words\"]\n",
    "test[\"num_words_title\"] = test[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))/test[\"num_words\"]\n",
    "test[\"chars_between_comma\"] = test[\"text\"].apply(lambda x: np.mean([len(chunk) for chunk in str(x).split(\",\")]))/test[\"num_chars\"]\n",
    "test[\"symbols_unknowns\"]=test[\"text\"].apply(lambda x: np.sum([not w in symbols_knowns for w in str(x)]))/test[\"num_chars\"]\n",
    "test['noun'] = test[\"text\"].apply(lambda x: fraction_noun(x))\n",
    "test['adj'] = test[\"text\"].apply(lambda x: fraction_adj(x))\n",
    "test['verbs'] = test[\"text\"].apply(lambda x: fraction_verbs(x))\n",
    "test[\"sentiment\"]=test[\"text\"].apply(sentiment_nltk)\n",
    "test['single_frac'] = test['text'].apply(lambda x: count_tokens(x, ['is', 'was', 'has', 'he', 'she', 'it', 'her', 'his']))/test[\"num_words\"]\n",
    "test['plural_frac'] = test['text'].apply(lambda x: count_tokens(x, ['are', 'were', 'have', 'we', 'they']))/test[\"num_words\"]\n",
    "test['first_word_len']=test['text'].apply(first_word_len)/test[\"num_chars\"]\n",
    "test['last_word_len']=test['text'].apply(last_word_len)/test[\"num_chars\"]\n",
    "test[\"first_word_id\"] = test['text'].apply(lambda x: symbol_id(list(x.strip())[0]))\n",
    "test[\"last_word_id\"] = test['text'].apply(lambda x: symbol_id(list(x.strip())[-1]))\n",
    "test['ease']=test['text'].apply(flesch_reading_ease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-funeral",
   "metadata": {},
   "source": [
    "### 작가별 등장인물의 list를 만들어서 text에 등장하는 인물과의 유사도를 계산하는 듯!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "planned-vegetable",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-be86deb95b5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mpersons_author_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_author_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mpersons_author_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_author_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mpersons_author_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_author_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mpersons_author_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_persons\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_author_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-be86deb95b5d>\u001b[0m in \u001b[0;36mget_persons\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mres_ne_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 품사를 태깅한 token을 개체명 인식하여 tree형태로 반환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mres_ne\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree2conlltags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_ne_tree\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# ne_chunk로 인해 tree로 반환된 것을 IOB 형태로 변환하여 줌.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mres_ne_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres_ne\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# IOB 형태가 된 token을 list 형태로 변환\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chunk\\__init__.py\u001b[0m in \u001b[0;36mne_chunk\u001b[1;34m(tagged_tokens, binary)\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[0mchunker_pickle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_MULTICLASS_NE_CHUNKER\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0mchunker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunker_pickle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mchunker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_tokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\chunk\\named_entity.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mEach\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0mshould\u001b[0m \u001b[0mbe\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtagged\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \"\"\"\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mtagged\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tagged_to_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mtag_one\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtagger\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_taggers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoose_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tag\\sequential.py\u001b[0m in \u001b[0;36mchoose_tag\u001b[1;34m(self, tokens, index, history)\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;31m# higher than that cutoff first; otherwise, return None.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cutoff_prob\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m         \u001b[0mpdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprob_classify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mprob_classify\u001b[1;34m(self, featureset)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mprob_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[0mfeature_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logarithmic\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\classify\\maxent.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, featureset, label)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;31m# Known feature name & value:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m                 \u001b[0mencoding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m             \u001b[1;31m# Otherwise, we might want to fire an \"unseen-value feature\".\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_persons(text):\n",
    "    def bind_names(tagged_words):\n",
    "        names=list()\n",
    "        name=list()\n",
    "        for i,w in enumerate(tagged_words): # 반복문 사용 시 몇 번째 반복문인지 확인 / 인덱스 번호와 컬렉션의 원소를 tuple형태로 반환 \n",
    "            if(\"PERSON\" in w[2]):\n",
    "                name.append(w[0])\n",
    "            else:\n",
    "                if(len(name)!=0):\n",
    "                    names.append(\" \".join(name))\n",
    "                name=list()\n",
    "                \n",
    "            if(i==len(tagged_words)-1 and len(name)!=0):\n",
    "                names.append(\" \".join(name))\n",
    "        return names                   \n",
    "\n",
    "    res_ne_tree = ne_chunk(pos_tag(word_tokenize(text))) # 품사를 태깅한 token을 개체명 인식하여 tree형태로 반환\n",
    "    res_ne = tree2conlltags(res_ne_tree) # ne_chunk로 인해 tree로 반환된 것을 IOB 형태로 변환하여 줌.\n",
    "    res_ne_list = [list(x) for x in res_ne] # IOB 형태가 된 token을 list 형태로 변환\n",
    "    return bind_names(res_ne_list)\n",
    "\n",
    "\n",
    "text_author_0 = \" \".join(list(train['text'][train['author']==0]))\n",
    "text_author_1 = \" \".join(list(train['text'][train['author']==1]))\n",
    "text_author_2 = \" \".join(list(train['text'][train['author']==2]))\n",
    "text_author_3 = \" \".join(list(train['text'][train['author']==3]))\n",
    "text_author_4 = \" \".join(list(train['text'][train['author']==4]))\n",
    "\n",
    "persons_author_0 = set(get_persons(text_author_0))\n",
    "persons_author_1 = set(get_persons(text_author_1))\n",
    "persons_author_2 = set(get_persons(text_author_2))\n",
    "persons_author_3 = set(get_persons(text_author_3))\n",
    "persons_author_4 = set(get_persons(text_author_4))\n",
    "\n",
    "# 자카드 지수(Jaccard index)는 두 집합 사이의 유사도를 측정하는 방법 중 하나이다. \n",
    "# 자카드 계수(Jaccard coefficient) 또는 자카드 유사도(Jaccard similarity)라고도 한다. \n",
    "# 자카드 지수는 0과 1 사이의 값을 가지며, 두 집합이 동일하면 1의 값을 가지고, 공통의 원소가 하나도 없으면 0의 값을 가진다.\n",
    "def jaccard(a,b):\n",
    "    return len(a&b)/len(a|b)\n",
    "\n",
    "# 작가별 등장 인물의 유사도를 feature로 넣는듯!!\n",
    "\n",
    "train[\"persons_0\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_0)) \n",
    "train[\"persons_1\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_1)) \n",
    "train[\"persons_2\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_2)) \n",
    "train[\"persons_3\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_3)) \n",
    "train[\"persons_4\"]=train[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_4)) \n",
    "\n",
    "test[\"persons_0\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_0)) \n",
    "test[\"persons_1\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_1)) \n",
    "test[\"persons_2\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_2)) \n",
    "test[\"persons_3\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_3)) \n",
    "test[\"persons_4\"]=test[\"text\"].apply(lambda x:jaccard(set(get_persons(x)),persons_author_4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-economics",
   "metadata": {},
   "source": [
    "## fasttext\n",
    "\n",
    "### festtext.train_unsupervised('data.txt')\n",
    "\n",
    "- 파라미터로 모델 지정이 가능함.('skipgram', 'cbow')\n",
    "- prameters\n",
    "    - input : training file path **(required)**\n",
    "    - model : unsupervised fasttext model / {cbow, skipgram} / default=skipgram\n",
    "    - lr : learning rate / default=0.05\n",
    "    - dim : size of word vectors / default=100\n",
    "    - ws : size of the context window / default=5\n",
    "    - epoch : number of epochs / default=5\n",
    "    - minCount : minimal number of word occurences / default=5\n",
    "    - minn : min length of char ngram / default=3\n",
    "    - maxn : min length of char ngram / default=6\n",
    "    - neg : number of negatives sampled / default=5\n",
    "    - wordNgrams : max length of wrd ngram / default=1\n",
    "    - loss : loss fuction / {ns, hs, softmax, ova} / default=ns\n",
    "    - bucket : number of buckets / default=2,000,000\n",
    "    - thread : number of treads / default=number of cpus\n",
    "    - lrUpdateRate : change the rate of updates for the learning rate / default=100\n",
    "    - t : sampling threshold / default=0.0001\n",
    "    - verbose : verbose / default=2\n",
    "- Model object fuctions\n",
    "    - get_dimension           \n",
    "        - Get the dimension (size) of a lookup vector (hidden layer).\n",
    "        - This is equivalent to `dim` property.\n",
    "    - get_input_vector        \n",
    "        - Given an index, get the corresponding vector of the Input Matrix.\n",
    "    - get_input_matrix        \n",
    "        - Get a copy of the full input matrix of a Model.\n",
    "    - get_labels              \n",
    "        - Get the entire list of labels of the dictionary\n",
    "        - This is equivalent to `labels` property.\n",
    "    - get_line                \n",
    "        - Split a line of text into words and labels.\n",
    "    - get_output_matrix       \n",
    "        - Get a copy of the full output matrix of a Model.\n",
    "    - get_sentence_vector     \n",
    "        - Given a string, get a single vector represenation. This function\n",
    "        - assumes to be given a single line of text. We split words on\n",
    "        - whitespace (space, newline, tab, vertical tab) and the control\n",
    "        - characters carriage return, formfeed and the null character.\n",
    "    - get_subword_id          \n",
    "        - Given a subword, return the index (within input matrix) it hashes to.\n",
    "    - get_subwords            \n",
    "        - Given a word, get the subwords and their indicies.\n",
    "    - get_word_id             \n",
    "        - Given a word, get the word id within the dictionary.\n",
    "    - get_word_vector         \n",
    "        - Get the vector representation of word.\n",
    "    - get_words               \n",
    "        - Get the entire list of words of the dictionary\n",
    "        - This is equivalent to `words` property.\n",
    "    - is_quantized            \n",
    "        - whether the model has been quantized\n",
    "    - predict                 \n",
    "        - Given a string, get a list of labels and a list of corresponding probabilities.\n",
    "    - quantize                \n",
    "        - Quantize the model reducing the size of the model and it's memory footprint.\n",
    "    - save_model              \n",
    "        - Save the model to the given path\n",
    "    - test                    \n",
    "        - Evaluate supervised model using file given by path\n",
    "    - test_label              \n",
    "        - Return the precision and recall score for each label.\n",
    "        \n",
    "### 실습\n",
    "~~~python\n",
    "import fasttext\n",
    "model = fasttext.train_unsupervised('review.sorted.uniq.refined.tsv.text.tok',model='skipgram', epoch=5,lr = 0.1)\n",
    "\n",
    "print(model['행사']) # get the vector of the word '행사'\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'].to_csv(data_path + 'sample_file.txt',index=False, header=None, sep=\"\\t\")\n",
    "model_ft = fasttext.train_unsupervised(data_path + 'sample_file.txt', minCount=2, minn=2, maxn=10,dim=300)\n",
    "\n",
    "def sent2vec(s):\n",
    "    words = nltk.tokenize.word_tokenize(s)\n",
    "    #words = [k.stem(w) for w in words]\n",
    "    #words = [w for w in words if not w in string.digits]\n",
    "    #words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model_ft[w]) # 해당 단어에 대한 vector를 추정.\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M) # vector를 array에 append\n",
    "    v = M.sum(axis=0) # axis=0을 기준으로 vector값을 더함. ==> 왜 이런 연산이 필요하지..?(하나의 vector로 만들어주려고..?)\n",
    "    if type(v) != np.ndarray: # 만들어진 해당 vector가 numpy의 array가 아닐 경우\n",
    "        return np.zeros(300) # 임의의 0으로 채워진 numpy array 반환\n",
    "    return v\n",
    "\n",
    "# train과 test의 각각 text data를 vector로 만들기\n",
    "xtrain_ft = np.array([sent2vec(x) for x in train['text']])\n",
    "xtest_ft = np.array([sent2vec(x) for x in test['text']])\n",
    "\n",
    "# 반환된 numpy arrya를 pandas dataframe으로 바꾸어주기\n",
    "train_ft=pd.DataFrame(xtrain_ft)\n",
    "# columns 명(feature명) 바꾸어 주기\n",
    "train_ft.columns = ['ft_vector_'+str(i) for i in range(xtrain_ft.shape[1])]\n",
    "\n",
    "# 반환된 numpy arrya를 pandas dataframe으로 바꾸어주기\n",
    "test_ft=pd.DataFrame(xtest_ft)\n",
    "# columns 명(feature명) 바꾸어 주기\n",
    "test_ft.columns = ['ft_vector_'+str(i) for i in range(xtrain_ft.shape[1])]\n",
    "\n",
    "# 기존의 data와 연결함.\n",
    "train = pd.concat([train, train_ft], axis=1)\n",
    "test = pd.concat([test, test_ft], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7640b640",
   "metadata": {},
   "source": [
    "## 각 sklearn.feature_extraction.text의 Vectorizer를 이용한 inference\n",
    "### Logistic Regression version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf20756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfidfVectorizer -> 문서목록에서 각 문서의 feature를 tf-idf 값을 가중치로 설정한 BOW 벡터를 만든다.\n",
    "# TF-IDF : TF-IDF(Term Frequency – Inverse Document Frequency)는 \n",
    "# 정보검색론(IR-Information Retrieval) 에선 흔하게 접하는 가중치를 구하는 알고리즘 입니다.\n",
    "\n",
    "# TfidfVectorizer를 정의\n",
    "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "# 값을 list로 변환하여 전달\n",
    "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
    "# train data로 fit되었으므로 그냥 transform, 마찬가지로 list로 변환하여 전달\n",
    "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
    "# label 값 분할\n",
    "train_y = train['author']\n",
    "\n",
    "# LogisticRegression의 ingerense 과정 정의\n",
    "def runLR(train_X,train_y,test_X,test_y,test_X2):\n",
    "    model=LogisticRegression()\n",
    "    model.fit(train_X,train_y) \n",
    "    pred_test_y=model.predict_proba(test_X) # 각 sample별로 해당 클래스일 확률을 계산하여 반환 (n_samples, n_classes)\n",
    "    pred_test_y2=model.predict_proba(test_X2) # 각 sample별로 해당 클래스일 확률을 계산하여 반환 (n_samples, n_classes)\n",
    "    return pred_test_y, pred_test_y2, model \n",
    "\n",
    "\n",
    "cv_scores=[]\n",
    "\n",
    "# training에 필요없는 feature drop\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "# label data 분할\n",
    "train_y=train['author']\n",
    "\n",
    "# inference에 필요없는 feature drop\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5]) # train_data의 row, num of classes(=5)\n",
    "\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y): # 나눈 fold의 index를 반환!\n",
    "    # fold 별로 분할\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "    # dev fold에 대해 inference\n",
    "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
    "    \n",
    "    print('test_y : ')\n",
    "    print(pred_test_y)\n",
    "    print('val_y : ')\n",
    "    print(pred_val_y)\n",
    "    \n",
    "    pred_full_test = pred_full_test + pred_test_y # TfidfVectorizer로 vector화한 vector에 대한 클래스별 확률\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    \n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "# TfidfVetorizer vector로 추론한 클래스 별 확률들의 평균 계산.\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "\n",
    "# 왜 추가하는지..?\n",
    "train[\"tfidf_LR_0\"] = pred_train[:,0] # class 0 일 확률을 feature로 추가\n",
    "train[\"tfidf_LR_1\"] = pred_train[:,1] # class 1 일 확률을 feature로 추가\n",
    "train[\"tfidf_LR_2\"] = pred_train[:,2] # class 2 일 확률을 feature로 추가\n",
    "train[\"tfidf_LR_3\"] = pred_train[:,3] # class 4 일 확률을 feature로 추가\n",
    "train[\"tfidf_LR_4\"] = pred_train[:,4] # class 5 일 확률을 feature로 추가\n",
    "test[\"tfidf_LR_0\"] = pred_full_test[:,0]\n",
    "test[\"tfidf_LR_1\"] = pred_full_test[:,1]\n",
    "test[\"tfidf_LR_2\"] = pred_full_test[:,2]\n",
    "test[\"tfidf_LR_3\"] = pred_full_test[:,3]\n",
    "test[\"tfidf_LR_4\"] = pred_full_test[:,4]\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# CountVectorizer : 오직 띄어쓰기만을 기준으로 단어를 자른 후에 BoW를 만든다.\n",
    "# 이는 영어의 경우 띄어쓰기만으로 토큰화가 수행되기 때문에 문제가 없지만\n",
    "# 한글에 CountVectorizer를 적용하면, 조사 등의 이유로 제대로 BoW가 만들어지지 않음을 의미합니다.\n",
    "# 예를 들어 \"봄과\"와 \"봄이\"를 다르게 인식하기 때문에 CountVectorizer 사용 전에 \n",
    "# 어간 추출을 해 \"봄\"으로 통일시켜 주는 것이 좋습니다.\n",
    "\n",
    "# 문서를 토큰 리스트로 변환하여 각 문서에서 토큰의 출현 빈도를 센다\n",
    "# 각 문서를 BOW 인코딩 벡터로 변환한다.\n",
    "\n",
    "# default -> 단어 n-그램\n",
    "cvec_vec=CountVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "cvec_vec.fit(train['text'].values.tolist())\n",
    "train_cvec = cvec_vec.transform(train['text'].values.tolist())\n",
    "test_cvec = cvec_vec.transform(test['text'].values.tolist())\n",
    "\n",
    "cv_scores=[]\n",
    "\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "\n",
    "train_y=train['author']\n",
    "\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_cvec[dev_index], train_cvec[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_cvec)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "\n",
    "# 왜 추가하는지..?\n",
    "train[\"cvec_LR_0\"] = pred_train[:,0]\n",
    "train[\"cvec_LR_1\"] = pred_train[:,1]\n",
    "train[\"cvec_LR_2\"] = pred_train[:,2]\n",
    "train[\"cvec_LR_3\"] = pred_train[:,3]\n",
    "train[\"cvec_LR_4\"] = pred_train[:,4]\n",
    "test[\"cvec_LR_0\"] = pred_full_test[:,0]\n",
    "test[\"cvec_LR_1\"] = pred_full_test[:,1]\n",
    "test[\"cvec_LR_2\"] = pred_full_test[:,2]\n",
    "test[\"cvec_LR_3\"] = pred_full_test[:,3]\n",
    "test[\"cvec_LR_4\"] = pred_full_test[:,4]\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# analyzer=char -> 문자 n-그램\n",
    "cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
    "cvec_char_vec.fit(train['text'].values.tolist())\n",
    "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
    "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
    "\n",
    "cv_scores=[]\n",
    "\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "\n",
    "train_y=train['author']\n",
    "\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runLR(dev_X, dev_y, val_X, val_y,test_cvec_char)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "\n",
    "# 왜 추가하는지..?\n",
    "train[\"cvec_char_LR_0\"] = pred_train[:,0]\n",
    "train[\"cvec_char_LR_1\"] = pred_train[:,1]\n",
    "train[\"cvec_char_LR_2\"] = pred_train[:,2]\n",
    "train[\"cvec_char_LR_3\"] = pred_train[:,3]\n",
    "train[\"cvec_char_LR_4\"] = pred_train[:,4]\n",
    "test[\"cvec_char_LR_0\"] = pred_full_test[:,0]\n",
    "test[\"cvec_char_LR_1\"] = pred_full_test[:,1]\n",
    "test[\"cvec_char_LR_2\"] = pred_full_test[:,2]\n",
    "test[\"cvec_char_LR_3\"] = pred_full_test[:,3]\n",
    "test[\"cvec_char_LR_4\"] = pred_full_test[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0e57f",
   "metadata": {},
   "source": [
    "## 각 sklearn.feature_extraction.text의 Vectorizer를 이용한 inference\n",
    "### SGDClassifier version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac1f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "\n",
    "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
    "train_y = train['author']\n",
    "\n",
    "def runSGD(train_X,train_y,test_X,test_y,test_X2):\n",
    "    model=SGDClassifier(loss='log')\n",
    "    model.fit(train_X,train_y)\n",
    "    pred_test_y=model.predict_proba(test_X)\n",
    "    pred_test_y2=model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "cv_scores=[]\n",
    "\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "\n",
    "train_y=train['author']\n",
    "\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runSGD(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# 왜 추가하는지...?\n",
    "train[\"tfidf_SGD_0\"] = pred_train[:,0]\n",
    "train[\"tfidf_SGD_1\"] = pred_train[:,1]\n",
    "train[\"tfidf_SGD_2\"] = pred_train[:,2]\n",
    "train[\"tfidf_SGD_3\"] = pred_train[:,3]\n",
    "train[\"tfidf_SGD_4\"] = pred_train[:,4]\n",
    "test[\"tfidf_SGD_0\"] = pred_full_test[:,0]\n",
    "test[\"tfidf_SGD_1\"] = pred_full_test[:,1]\n",
    "test[\"tfidf_SGD_2\"] = pred_full_test[:,2]\n",
    "test[\"tfidf_SGD_3\"] = pred_full_test[:,3]\n",
    "test[\"tfidf_SGD_4\"] = pred_full_test[:,4]\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
    "cvec_char_vec.fit(train['text'].values.tolist())\n",
    "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
    "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
    "\n",
    "cv_scores=[]\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "train_y=train['author']\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runSGD(dev_X, dev_y, val_X, val_y,test_cvec_char)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "# 왜 추가하는지...?\n",
    "train[\"cvec_char_SGD_0\"] = pred_train[:,0]\n",
    "train[\"cvec_char_SGD_1\"] = pred_train[:,1]\n",
    "train[\"cvec_char_SGD_2\"] = pred_train[:,2]\n",
    "train[\"cvec_char_SGD_3\"] = pred_train[:,3]\n",
    "train[\"cvec_char_SGD_4\"] = pred_train[:,4]\n",
    "test[\"cvec_char_SGD_0\"] = pred_full_test[:,0]\n",
    "test[\"cvec_char_SGD_1\"] = pred_full_test[:,1]\n",
    "test[\"cvec_char_SGD_2\"] = pred_full_test[:,2]\n",
    "test[\"cvec_char_SGD_3\"] = pred_full_test[:,3]\n",
    "test[\"cvec_char_SGD_4\"] = pred_full_test[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ed23a",
   "metadata": {},
   "source": [
    "## 각 sklearn.feature_extraction.text의 Vectorizer를 이용한 inference\n",
    "### RandomForestClassifier version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d457e23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "\n",
    "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
    "train_y = train['author']\n",
    "\n",
    "def runRF(train_X,train_y,test_X,test_y,test_X2):\n",
    "    model=RandomForestClassifier()\n",
    "    model.fit(train_X,train_y)\n",
    "    pred_test_y=model.predict_proba(test_X)\n",
    "    pred_test_y2=model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "\n",
    "cv_scores=[]\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "train_y=train['author']\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runRF(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "train[\"tfidf_RF_0\"] = pred_train[:,0]\n",
    "train[\"tfidf_RF_1\"] = pred_train[:,1]\n",
    "train[\"tfidf_RF_2\"] = pred_train[:,2]\n",
    "train[\"tfidf_RF_3\"] = pred_train[:,3]\n",
    "train[\"tfidf_RF_4\"] = pred_train[:,4]\n",
    "test[\"tfidf_RF_0\"] = pred_full_test[:,0]\n",
    "test[\"tfidf_RF_1\"] = pred_full_test[:,1]\n",
    "test[\"tfidf_RF_2\"] = pred_full_test[:,2]\n",
    "test[\"tfidf_RF_3\"] = pred_full_test[:,3]\n",
    "test[\"tfidf_RF_4\"] = pred_full_test[:,4]\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
    "cvec_char_vec.fit(train['text'].values.tolist())\n",
    "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
    "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
    "\n",
    "cv_scores=[]\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "train_y=train['author']\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runRF(dev_X, dev_y, val_X, val_y,test_cvec_char)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "train[\"cvec_char_RF_0\"] = pred_train[:,0]\n",
    "train[\"cvec_char_RF_1\"] = pred_train[:,1]\n",
    "train[\"cvec_char_RF_2\"] = pred_train[:,2]\n",
    "train[\"cvec_char_RF_3\"] = pred_train[:,3]\n",
    "train[\"cvec_char_RF_4\"] = pred_train[:,4]\n",
    "test[\"cvec_char_RF_0\"] = pred_full_test[:,0]\n",
    "test[\"cvec_char_RF_1\"] = pred_full_test[:,1]\n",
    "test[\"cvec_char_RF_2\"] = pred_full_test[:,2]\n",
    "test[\"cvec_char_RF_3\"] = pred_full_test[:,3]\n",
    "test[\"cvec_char_RF_4\"] = pred_full_test[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb6caa",
   "metadata": {},
   "source": [
    "## 각 sklearn.feature_extraction.text의 Vectorizer를 이용한 inference\n",
    "### MLPClassifier version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310af5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "\n",
    "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
    "train_y = train['author']\n",
    "\n",
    "def runMLP(train_X,train_y,test_X,test_y,test_X2):\n",
    "    model=MLPClassifier()\n",
    "    model.fit(train_X,train_y)\n",
    "    pred_test_y=model.predict_proba(test_X)\n",
    "    pred_test_y2=model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "\n",
    "cv_scores=[]\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "train_y=train['author']\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMLP(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "train[\"tfidf_MLP_0\"] = pred_train[:,0]\n",
    "train[\"tfidf_MLP_1\"] = pred_train[:,1]\n",
    "train[\"tfidf_MLP_2\"] = pred_train[:,2]\n",
    "train[\"tfidf_MLP_3\"] = pred_train[:,3]\n",
    "train[\"tfidf_MLP_4\"] = pred_train[:,4]\n",
    "test[\"tfidf_MLP_0\"] = pred_full_test[:,0]\n",
    "test[\"tfidf_MLP_1\"] = pred_full_test[:,1]\n",
    "test[\"tfidf_MLP_2\"] = pred_full_test[:,2]\n",
    "test[\"tfidf_MLP_3\"] = pred_full_test[:,3]\n",
    "test[\"tfidf_MLP_4\"] = pred_full_test[:,4]\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "cvec_char_vec = CountVectorizer(ngram_range=(1,5), analyzer='char')\n",
    "cvec_char_vec.fit(train['text'].values.tolist())\n",
    "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
    "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
    "train_y = train['author']\n",
    "\n",
    "cv_scores=[]\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "train_y=train['author']\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMLP(dev_X, dev_y, val_X, val_y,test_cvec_char)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "train[\"cvec_char_MLP_0\"] = pred_train[:,0]\n",
    "train[\"cvec_char_MLP_1\"] = pred_train[:,1]\n",
    "train[\"cvec_char_MLP_2\"] = pred_train[:,2]\n",
    "train[\"cvec_char_MLP_3\"] = pred_train[:,3]\n",
    "train[\"cvec_char_MLP_4\"] = pred_train[:,4]\n",
    "test[\"cvec_char_MLP_0\"] = pred_full_test[:,0]\n",
    "test[\"cvec_char_MLP_1\"] = pred_full_test[:,1]\n",
    "test[\"cvec_char_MLP_2\"] = pred_full_test[:,2]\n",
    "test[\"cvec_char_MLP_3\"] = pred_full_test[:,3]\n",
    "test[\"cvec_char_MLP_4\"] = pred_full_test[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fc7954",
   "metadata": {},
   "source": [
    "## 각 sklearn.feature_extraction.text의 Vectorizer를 이용한 inference\n",
    "### DecisionTreeClassifier version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93571a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\n",
    "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
    "train_y = train['author']\n",
    "\n",
    "def runDT(train_X,train_y,test_X,test_y,test_X2):\n",
    "    model=DecisionTreeClassifier()\n",
    "    model.fit(train_X,train_y)\n",
    "    pred_test_y=model.predict_proba(test_X)\n",
    "    pred_test_y2=model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "cv_scores=[]\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "train_y=train['author']\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runDT(dev_X, dev_y, val_X, val_y,test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "train[\"tfidf_DT_0\"] = pred_train[:,0]\n",
    "train[\"tfidf_DT_1\"] = pred_train[:,1]\n",
    "train[\"tfidf_DT_2\"] = pred_train[:,2]\n",
    "train[\"tfidf_DT_3\"] = pred_train[:,3]\n",
    "train[\"tfidf_DT_4\"] = pred_train[:,4]\n",
    "test[\"tfidf_DT_0\"] = pred_full_test[:,0]\n",
    "test[\"tfidf_DT_1\"] = pred_full_test[:,1]\n",
    "test[\"tfidf_DT_2\"] = pred_full_test[:,2]\n",
    "test[\"tfidf_DT_3\"] = pred_full_test[:,3]\n",
    "test[\"tfidf_DT_4\"] = pred_full_test[:,4]\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
    "cvec_char_vec.fit(train['text'].values.tolist())\n",
    "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
    "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
    "train_y = train['author']\n",
    "\n",
    "cv_scores=[]\n",
    "cols_to_drop=['text','index']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "train_y=train['author']\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "pred_full_test = 0\n",
    "\n",
    "cv = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in cv.split(train_X,train_y):\n",
    "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runDT(dev_X, dev_y, val_X, val_y,test_cvec_char)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "train[\"cvec_char_DT_0\"] = pred_train[:,0]\n",
    "train[\"cvec_char_DT_1\"] = pred_train[:,1]\n",
    "train[\"cvec_char_DT_2\"] = pred_train[:,2]\n",
    "train[\"cvec_char_DT_3\"] = pred_train[:,3]\n",
    "train[\"cvec_char_DT_4\"] = pred_train[:,4]\n",
    "test[\"cvec_char_DT_0\"] = pred_full_test[:,0]\n",
    "test[\"cvec_char_DT_1\"] = pred_full_test[:,1]\n",
    "test[\"cvec_char_DT_2\"] = pred_full_test[:,2]\n",
    "test[\"cvec_char_DT_3\"] = pred_full_test[:,3]\n",
    "test[\"cvec_char_DT_4\"] = pred_full_test[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08874a10",
   "metadata": {},
   "source": [
    "## TruncatedSVD\n",
    "\n",
    "- singular value decomposition(SVD)를 이용하여 linear dimensinality reduction(선형 차원 축소)를 수행함.\n",
    "- PCA와는 다르게 이 estimator는 singular value decomposition 값을 계산하기 이전에 data를 center에 두지 않음.\n",
    "    - 이는 희소 행렬에서 효율적으로 동작한다는 것을 의미한다.\n",
    "- 특히, turncated SVD는 sklearn.feature_extraction.text의 벡터라이저가 반환한 term count/tf-idf 행렬에서 작용함.\n",
    "    - 이 맥락에서 latent semantic analysis(LSA, 잠재 의미 분석)으로 알려져 있음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ae516",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec=TfidfVectorizer(stop_words='english',ngram_range=(1,3))\n",
    "train_tfidf= tfidf_vec.fit_transform(train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
    "\n",
    "\n",
    "\n",
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(train_tfidf)\n",
    "\n",
    "# 선형 차원 축소 진행\n",
    "train_svd = svd_obj.transform(train_tfidf)\n",
    "test_svd = svd_obj.transform(test_tfidf)\n",
    "\n",
    "# 선형 차원 축소한 data를 표준화\n",
    "from sklearn import preprocessing\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(train_svd)\n",
    "train_svd_scl = pd.DataFrame(scl.transform(train_svd))\n",
    "test_svd_scl = pd.DataFrame(scl.transform(test_svd))\n",
    "\n",
    "# 각 dataframe의 comlumn 명 정의\n",
    "train_svd_scl.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd_scl.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "\n",
    "# train과 test에 해당 데이터를 concat\n",
    "train = pd.concat([train, train_svd_scl], axis=1)\n",
    "test = pd.concat([test, test_svd_scl], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5285c3df",
   "metadata": {},
   "source": [
    "## naive_bayes.MultinomialNB\n",
    "\n",
    "- 다항식 모델을 위한 Naive bayes classifier\n",
    "- 이는 이산형 특징을 가진 분류(ex: 텍스트 분류를 위한 단어 수 세기)에 적합함.\n",
    "- 다항 분포에는 일반적으로 정수 피쳐 카운트가 필요함.\n",
    "- 그러나 실제로는 tf-idf와 같은 부분 계수도 작동할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab609e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMNB(train_X,train_y,test_X,test_y,test_X2):\n",
    "    model=naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X,train_y)\n",
    "    pred_test_y=model.predict_proba(test_X)\n",
    "    pred_test_y2=model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n",
    "\n",
    "Count_vec=CountVectorizer(stop_words='english',ngram_range=(1,3))\n",
    "\n",
    "Count_vec.fit(train['text'].values.tolist())\n",
    "train_Count = Count_vec.transform(train['text'].values.tolist())\n",
    "test_Count = Count_vec.transform(test['text'].values.tolist())\n",
    "\n",
    "cv_scores=[]\n",
    "pred_train=np.zeros([train.shape[0],5])\n",
    "pred_full_test = 0\n",
    "\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_Count[dev_index], train_Count[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y,test_Count)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "train[\"nb_cvec_0\"] = pred_train[:,0]\n",
    "train[\"nb_cvec_1\"] = pred_train[:,1]\n",
    "train[\"nb_cvec_2\"] = pred_train[:,2]\n",
    "train[\"nb_cvec_3\"] = pred_train[:,3]\n",
    "train[\"nb_cvec_4\"] = pred_train[:,4]\n",
    "test[\"nb_cvec_0\"] = pred_full_test[:,0]\n",
    "test[\"nb_cvec_1\"] = pred_full_test[:,1]\n",
    "test[\"nb_cvec_2\"] = pred_full_test[:,2]\n",
    "test[\"nb_cvec_3\"] = pred_full_test[:,3]\n",
    "test[\"nb_cvec_4\"] = pred_full_test[:,4]\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "cvec_char_vec = CountVectorizer(ngram_range=(1,7), analyzer='char')\n",
    "cvec_char_vec.fit(train['text'].values.tolist())\n",
    "train_cvec_char = cvec_char_vec.transform(train['text'].values.tolist())\n",
    "test_cvec_char = cvec_char_vec.transform(test['text'].values.tolist())\n",
    "\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_cvec_char[dev_index], train_cvec_char[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_cvec_char)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "train[\"nb_cvec_char_0\"] = pred_train[:,0]\n",
    "train[\"nb_cvec_char_1\"] = pred_train[:,1]\n",
    "train[\"nb_cvec_char_2\"] = pred_train[:,2]\n",
    "train[\"nb_cvec_char_3\"] = pred_train[:,3]\n",
    "train[\"nb_cvec_char_4\"] = pred_train[:,4]\n",
    "test[\"nb_cvec_char_0\"] = pred_full_test[:,0]\n",
    "test[\"nb_cvec_char_1\"] = pred_full_test[:,1]\n",
    "test[\"nb_cvec_char_2\"] = pred_full_test[:,2]\n",
    "test[\"nb_cvec_char_3\"] = pred_full_test[:,3]\n",
    "test[\"nb_cvec_char_4\"] = pred_full_test[:,4]\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\n",
    "\n",
    "train_tfidf = tfidf_vec.fit_transform(train['text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test['text'].values.tolist())\n",
    "\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train.shape[0], 5])\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "pred_full_test = pred_full_test / 5.\n",
    "\n",
    "\n",
    "train[\"nb_tfidf_char_0\"] = pred_train[:,0]\n",
    "train[\"nb_tfidf_char_1\"] = pred_train[:,1]\n",
    "train[\"nb_tfidf_char_2\"] = pred_train[:,2]\n",
    "train[\"nb_tfidf_char_3\"] = pred_train[:,3]\n",
    "train[\"nb_tfidf_char_4\"] = pred_train[:,4]\n",
    "test[\"nb_tfidf_char_0\"] = pred_full_test[:,0]\n",
    "test[\"nb_tfidf_char_1\"] = pred_full_test[:,1]\n",
    "test[\"nb_tfidf_char_2\"] = pred_full_test[:,2]\n",
    "test[\"nb_tfidf_char_3\"] = pred_full_test[:,3]\n",
    "test[\"nb_tfidf_char_4\"] = pred_full_test[:,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8b673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(train_tfidf)\n",
    "\n",
    "train_svd = svd_obj.transform(train_tfidf)\n",
    "test_svd = svd_obj.transform(test_tfidf)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scl = preprocessing.StandardScaler()\n",
    "scl.fit(train_svd)\n",
    "train_svd_scl = pd.DataFrame(scl.transform(train_svd))\n",
    "test_svd_scl = pd.DataFrame(scl.transform(test_svd))\n",
    "\n",
    "train_svd_scl.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
    "test_svd_scl.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
    "train = pd.concat([train, train_svd_scl], axis=1)\n",
    "test = pd.concat([test, test_svd_scl], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1ffb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## XGBoost를 이용한 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0cac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['index', 'text']\n",
    "train_X = train.drop(cols_to_drop+['author'], axis=1)\n",
    "train_y=train['author']\n",
    "test_index = test['index'].values\n",
    "test_X = test.drop(cols_to_drop, axis=1)\n",
    "xgb_preds=[]\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    \n",
    "    dtrain = xgb.DMatrix(dev_X,label=dev_y)\n",
    "    dvalid = xgb.DMatrix(val_X, label=val_y)\n",
    "    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n",
    "\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 3\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 5\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = 0.3\n",
    "    param['seed'] = 0\n",
    "    param['tree_method'] = 'gpu_hist'\n",
    "\n",
    "    model = xgb.train(param, dtrain, 2000, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "\n",
    "    xgtest2 = xgb.DMatrix(test_X)\n",
    "    xgb_pred = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    xgb_preds.append(list(xgb_pred))\n",
    "\n",
    "#out_df = pd.DataFrame(pred_full_test)\n",
    "#out_df.columns = ['0','1','2','3','4']\n",
    "#out_df.insert(0, 'index', test_index)\n",
    "#out_df.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671091c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "xgb.plot_importance(model, max_num_features=80, height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c219f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(xgb_preds[0])):\n",
    "    sum=0\n",
    "    for j in range(5):\n",
    "        sum+=xgb_preds[j][i]    \n",
    "    if(i==0):\n",
    "        preds=sum/5\n",
    "    else:\n",
    "        preds=np.vstack([preds,sum/5])\n",
    "\n",
    "preds=pd.DataFrame(preds)\n",
    "\n",
    "preds.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
